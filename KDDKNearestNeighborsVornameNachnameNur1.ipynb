{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klassifikation mit *k*-Nearest-Neighbors\n",
    "Ziel dieser Übung ist das eigenständige Implementieren des Klassifikations-Algorithmus *k*-Nearest-Neighbors innerhalb des Jupyter-Notebooks. Implementieren sie folgende Variante des *k*-Nearest-Neighbors:\n",
    "- Alle Attribute sind vor der Benutzung auf den Wertebereich $[0;1]$ zu normieren. Beachten Sie dabei, dass für das \"Training\" des Klassifikators keine Informationen aus den Testdaten verwendet werden dürfen.\n",
    "- Als Distanzfunktion nutzen Sie bitte die euklidische Distanz.\n",
    "- Für das Abstimmen der *k* nächsten Nachbarn soll es vier Varianten geben, die mittels eines Parameters an die Klasse übergeben werden können:\n",
    "    1. Eine einfache Mehrheitsabstimmung unter den Nachbarn\n",
    "    2. Jeder Nachbar wird mit dem inversen Quadrat der Distanz gewichtet.\n",
    "    3. Die Stimmen einer Klasse werden mit dem Inversen ihrer Durchschnittsdistanz gewichtet.\n",
    "    4. Eine Mehrheitsabstimmung gewichtet nach der Verteilung der Klassen.\n",
    "   \n",
    "Sie dürfen die Pakete **collections**, **math** und **numpy** für Ihre Implementierung nutzen. Für die Ausführung der Tests benötigen Sie außerdem **pandas**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import random # Just needed for 6ECTS\n",
    "\n",
    "\n",
    "def euclidean_dist(x, y):\n",
    "     # TODO\n",
    "    #prüfen ob die beiden Vektoren x und y die gleiche Dimension \n",
    "    assert len(x) == len(y)\n",
    "    #die Summe der quadrierten Differenzen berechnet \n",
    "    squared_dist = sum((xi - yi) ** 2 for xi, yi in zip(x, y))\n",
    "    return math.sqrt(squared_dist)\n",
    "\n",
    "\n",
    "class KNN():\n",
    "    def __init__(self, dist_fun=euclidean_dist):\n",
    "        self.dist_fun = dist_fun\n",
    "        self.strategies = ['majority', 'inverse_squared_distance', 'inverse_avg_distance', 'distribution']\n",
    "    \n",
    "    def train(self, X, Y, ks=None):\n",
    "        \"\"\" Train this classifier. Takes a list of samples X and a list of class-labels Y.\n",
    "        Each sample is a list of numeric values. Each label is a string.\n",
    "        The parameter ks ist just needed for 6ECTS.\"\"\"\n",
    "        assert(len(X) == len(Y))\n",
    "        \"\"\" Train this classifier. Takes a list of samples X and a list of class-labels Y.\n",
    "        Each sample is a list of numeric values. Each label is a string.\n",
    "        The parameter ks ist just needed for 6ECTS.\"\"\"\n",
    "        #x die numerische Daten,y labels/Bezeichnung, was macht gibt zurück keine Ahnung\n",
    "        assert(len(X) == len(Y))\n",
    "        \n",
    "        # TODO\n",
    "        #auf den Wertebereich [0;1] zu normieren\n",
    "        data = np.array(X)\n",
    "        normalizedData = (data-np.min(data))/(np.max(data)-np.min(data))\n",
    "\n",
    "        neighbors=[]\n",
    "        #Eine einfache Mehrheitsabstimmung unter den Nachbarn\n",
    "        \n",
    "        # Jeder Nachbar wird mit dem inversen Quadrat der Distanz gewichtet.\n",
    "        \n",
    "        # Die Stimmen einer Klasse werden mit dem Inversen ihrer Durchschnittsdistanz gewichtet.\n",
    "        \n",
    "        # Eine Mehrheitsabstimmung gewichtet nach der Verteilung der Klassen.\n",
    "        \n",
    "      \n",
    "    # Eine einfache Mehrheitsabstimmung unter den Nachbarn\n",
    "    # Stimmen werden gleich gewichtet\n",
    "    if strategy == 'majority':\n",
    "        for i, x in enumerate(normalizedData):\n",
    "            distances = [(self.dist_fun(x, x2), y) for x2, y in zip(normalizedData, Y)]\n",
    "            sorted_distances = sorted(distances)\n",
    "            neighbors.append(sorted_distances[:k])\n",
    "\n",
    "    # Jeder Nachbar wird mit dem inversen Quadrat der Distanz gewichtet\n",
    "    elif strategy == 'inverse_squared_distance':\n",
    "        for i, x in enumerate(normalizedData):\n",
    "            distances = [(self.dist_fun(x, x2), y) for x2, y in zip(normalizedData, Y)]\n",
    "            sorted_distances = sorted(distances)\n",
    "            # Exponentielle Gewichtung der Distanzen\n",
    "            neighbors.append([(1 / d**2, y) for d, y in sorted_distances[:k]])\n",
    "\n",
    "    # Die Stimmen einer Klasse werden mit dem Inversen ihrer Durchschnittsdistanz gewichtet\n",
    "    elif strategy == 'inverse_avg_distance':\n",
    "        for i, x in enumerate(normalizedData):\n",
    "            distances = [(self.dist_fun(x, x2), y) for x2, y in zip(normalizedData, Y)]\n",
    "            sorted_distances = sorted(distances)\n",
    "            # Berechnung der durchschnittlichen Distanz zur jeder Klasse\n",
    "            avg_distances = defaultdict(float)\n",
    "            for d, y in sorted_distances[:k]:\n",
    "                avg_distances[y] += d\n",
    "            for y in avg_distances:\n",
    "                avg_distances[y] /= k\n",
    "            # Exponentielle Gewichtung der Distanzen\n",
    "            neighbors.append([(1 / d, y) for y, d in avg_distances.items()])\n",
    "\n",
    "    # Eine Mehrheitsabstimmung gewichtet nach der Verteilung der Klassen\n",
    "    elif strategy == 'distribution':\n",
    "        for i, x in enumerate(normalizedData):\n",
    "            distances = [(self.dist_fun(x, x2), y) for x2, y in zip(normalizedData, Y)]\n",
    "            sorted_distances = sorted(distances)\n",
    "            # Berechnung der Gewichte basierend auf der Verteilung der Klassen\n",
    "            class_counts = Counter(y for _, y in sorted_distances[:k])\n",
    "            total_count = sum(class_counts.values())\n",
    "            weights = {y: (1 - (class_counts[y] / total_count)) for y in class_counts}\n",
    "            # Gewichtete Stimmabgabe\n",
    "            neighbors.append([(weights[y], y) for _, y in sorted_distances[:k]])\n",
    "\n",
    "    return neighbors\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return neighbors\n",
    "    \n",
    "    def predict(self, X, k=3, strategy='majority', best_combination=False):\n",
    "        \"\"\" Takes a list of samples X. Returns a list of predicted labels for the samples.\n",
    "        The parameter best_combination ist just needed for 6ECTS.\"\"\"\n",
    "        return [self.predict_sample(x, k, strategy) for x in X]\n",
    "    \n",
    "    def predict_sample(self, x, k=3, strategy='majority'):\n",
    "        \"\"\" Predicts the label for a single sample x. \"\"\"\n",
    "        \n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluierung des Klassifikators\n",
    "Mit diesem Code können Sie Ihre Implementierung anhand des mitgelieferten IRIS-Datensatzes testen. Probieren Sie auch verschiedene (sinnvolle) Werte für den Parameter *k*. Bitte ansonsten in diesem Teil nichts mehr ändern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, targets):\n",
    "    \"\"\" Calculates the accuracy for the given class predictions and true classes.\"\"\"\n",
    "    assert(len(predictions) == len(targets))\n",
    "    n_correct = len([p for p,t in zip(predictions, targets) if p==t])\n",
    "    return n_correct/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(predictions, targets):\n",
    "    \"\"\" Returns a tuple (labels, m) where m is the confusion matrix and \n",
    "    labels is the list of matrix rows/columns in same order as in the matrix.\n",
    "    Rows in the confusion matrix indicate the true target label\n",
    "    whereas the columns indicate the predicted label of samples. \"\"\"\n",
    "    assert(len(predictions) == len(targets))\n",
    "    \n",
    "    # Map each label to an index.\n",
    "    unique_vals = list(set(predictions).union(targets))\n",
    "    mapping = {label: index for index, label in enumerate(unique_vals)}\n",
    "    \n",
    "    # Build and fill the confusion matrix.\n",
    "    m = [[0]*len(mapping) for _ in range(len(mapping))]\n",
    "    for p, t in zip(predictions, targets):\n",
    "        row, col = mapping[t], mapping[p]\n",
    "        m[row][col] += 1\n",
    "    return unique_vals, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the csv and drop duplicate entries.\n",
    "data = pd.read_csv('iris.csv').drop_duplicates()\n",
    "\n",
    "# Draw a random sample without replacement for the test data.\n",
    "test_data = data.sample(n=50)\n",
    "\n",
    "# The other samples are used as training data.\n",
    "train_data = data.loc[data.index.drop(test_data.index), :]\n",
    "\n",
    "def df_to_vectors(df):\n",
    "    \"\"\"Takes a pandas data-frame from the iris dataset as input.\n",
    "    Returns a tuple (X, Y) where Y is a list of class labels and X is the list of sample-vectors\n",
    "    with each vector represented as a list of numeric values.\"\"\"\n",
    "    df = df.copy()\n",
    "    classes = df['species']\n",
    "    del df['species']\n",
    "    return df.values.tolist(), classes.values.tolist()\n",
    "\n",
    "# Convert train and test-data to lists of vectors and class labels.\n",
    "X_train, Y_train = df_to_vectors(train_data)\n",
    "X_test, Y_test = df_to_vectors(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNN()\n",
    "clf.train(X_train, Y_train)\n",
    "for strategy in clf.strategies:\n",
    "    predictions = clf.predict(X_test, strategy=strategy, k=3)\n",
    "    print('Accuracy of strategy {}: {}'.format(strategy, accuracy(predictions, Y_test)))\n",
    "    labels, matrix = confusion_matrix(predictions, Y_test)\n",
    "    print('Confusion matrix:')\n",
    "    print('\\n'.join([str(row) for row in matrix]))\n",
    "    print('----------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
